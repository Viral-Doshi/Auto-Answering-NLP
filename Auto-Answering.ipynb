{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTO GENERATION OF SUBJECTIVE ANSWERS\n",
    "\n",
    "### Step 1:  Raw Text Data to Organized dataframe\n",
    "### Step 2:  Paragraph-wise Keyword Extraction\n",
    "### Step 3:  Vectorizing Keywords to form Representative Vectors for paragraphs\n",
    "### Step 4:  Summarizing Paragraphs to generate fixed length Answers\n",
    "### Step 5:  Query Question to Vector\n",
    "### Step 6:  Cosine Similarity to find required paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import yake\n",
    "import nltk\n",
    "import string\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from rake_nltk import Rake\n",
    "from keybert import KeyBERT\n",
    "from termcolor import colored\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from sumy.utils import get_stop_words\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from __future__ import absolute_import\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from gensim.summarization import keywords\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from gensim.summarization import keywords\n",
    "from gensim.summarization import summarize\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.edmundson import EdmundsonSummarizer\n",
    "from __future__ import division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Step1: Raw Text Data to Organized dataframe</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_data = [[]]\n",
    "para_name = ''\n",
    "chapter_name = ''\n",
    "EndOfChap = 0\n",
    "file = \"Our_Past_1_Chapter2.txt\"\n",
    "with open(file, encoding=\"utf8\") as f:\n",
    "    for count, line in enumerate(f):\n",
    "        if EndOfChap < 2:\n",
    "            if 'CHAPTER' in line:\n",
    "                chapter_name = line.split(':')[1].strip()\n",
    "                continue\n",
    "                \n",
    "            if line == '\\n':\n",
    "                para_data.append([])\n",
    "                EndOfChap +=1\n",
    "                \n",
    "            elif para_data[-1] == [] and para_name == '':\n",
    "                para_name = line\n",
    "                \n",
    "            else:\n",
    "                para_descr = line\n",
    "                para_data[-1] = [chapter_name, para_name, para_descr]\n",
    "                para_name = ''\n",
    "                EndOfChap  = 0\n",
    "                \n",
    "\n",
    "para_data = [i for i in para_data if i != []]\n",
    "for o_index in range(len(para_data)):\n",
    "    for i_index in range(len(para_data[o_index])):\n",
    "        para_data[o_index][i_index] = para_data[o_index][i_index].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "count = 0\n",
    "sent_data = []\n",
    "para_name = ''\n",
    "chapter_name = ''\n",
    "EndOfChap = 0\n",
    "newpara = False\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "with open(file, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        a.append(line)\n",
    "\n",
    "for index,line in enumerate(a):\n",
    "    if EndOfChap < 2:\n",
    "        if 'CHAPTER' in line:\n",
    "            chapter_name = line.split(':')[1].strip()\n",
    "            continue\n",
    "            \n",
    "        if line == '\\n':\n",
    "            para_name = ''\n",
    "            newpara = True\n",
    "            EndOfChap +=1\n",
    "            continue\n",
    "        \n",
    "        if newpara and para_name == '':\n",
    "            para_name = line.strip()\n",
    "            newpara = False\n",
    "            continue\n",
    "            \n",
    "        if not newpara:\n",
    "            doc = nlp(a[index])\n",
    "            for sent in doc.sents:\n",
    "                fsent = str(sent).strip()\n",
    "                sent_data.append([chapter_name,para_name,fsent])\n",
    "            EndOfChap  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent = pd.DataFrame(sent_data, columns = ['Chapter Name', 'Paragraph Title', 'Sentence'])\n",
    "df_para = pd.DataFrame(para_data, columns = ['Chapter Name', 'Paragraph Title', 'Description'])\n",
    "df_para['Description'] = df_para['Description'].apply(lambda text: text.translate(str.maketrans('', '', string.punctuation)))\n",
    "df_sent.index = np.arange(1,len(df_sent)+1)\n",
    "df_para.index = np.arange(1,len(df_para)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Step 2: Paragraph-wise Keyword Extraction</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_kw_ext(text):\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    result = []\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN']\n",
    "    doc = nlp(text.lower())\n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            result.append(token.text)\n",
    "\n",
    "    final_kw =  [(x[0]) for x in Counter(result).most_common(10)]\n",
    "    return final_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yake_kw_ext(text):\n",
    "    kw_extractor = yake.KeywordExtractor()\n",
    "    final_kw = []\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 1\n",
    "    deduplication_threshold = 0.9\n",
    "    numOfKeywords = 10\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    for kw in keywords[::-1]:\n",
    "        final_kw.append(kw[0])\n",
    "    return final_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rakenltk_kw_ext(text):\n",
    "    rake_nltk_var = Rake(min_length=1, max_length=1)\n",
    "    rake_nltk_var.extract_keywords_from_text(text)\n",
    "    keyword_extracted = rake_nltk_var.get_ranked_phrases()\n",
    "    return keyword_extracted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_kw_ext(text):\n",
    "    k = keywords(text, words = 10, lemmatize = True,).split('\\n')\n",
    "    final_kw = []\n",
    "    for i in k:\n",
    "        final_kw+=i.split(' ')\n",
    "    return final_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KeyBert_distilbert(text):\n",
    "    kw = []\n",
    "    kw_model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "    keywords = kw_model.extract_keywords(text)\n",
    "    temp = kw_model.extract_keywords(text, keyphrase_ngram_range=(1,1), stop_words='english', use_maxsum=True, top_n=10, diversity=0.2, nr_candidates = 20)\n",
    "    for i in temp:\n",
    "        kw.append(i[0])\n",
    "    return kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KeyBert_paraphrase(text):\n",
    "    kw = []\n",
    "    kw_model = KeyBERT(model='xlm-r-distilroberta-base-paraphrase-v1')\n",
    "    keywords = kw_model.extract_keywords(text)\n",
    "    temp = kw_model.extract_keywords(text, keyphrase_ngram_range=(1,1), stop_words='english', use_maxsum=True, diversity=0.7, top_n = 10)\n",
    "    for i in temp:\n",
    "        kw.append(i[0])\n",
    "    return kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords = pd.DataFrame()\n",
    "df_keywords['Spacy'] = df_para['Description'].apply(spacy_kw_ext)\n",
    "df_keywords['Yake'] = df_para['Description'].apply(yake_kw_ext)\n",
    "df_keywords['RakeNLTK'] = df_para['Description'].apply(rakenltk_kw_ext)\n",
    "df_keywords['Gensim'] = df_para['Description'].apply(gensim_kw_ext)\n",
    "df_keywords['Keybert-distilbert'] = df_para['Description'].apply(KeyBert_distilbert)\n",
    "df_keywords['Keybert-paraphrase'] = df_para['Description'].apply(KeyBert_paraphrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Step 3: Vectorizing Keywords to form Representative Vectors for paragraps</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file,encoding = 'utf8') as f:  \n",
    "    text = f.read()\n",
    "    processed_text = text.lower()\n",
    "    processed_text = processed_text.replace(\"-\", \"\")\n",
    "    processed_text = re.sub('[^a-zA-Z]', ' ', processed_text)\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
    "\n",
    "all_sentences = nltk.sent_tokenize(processed_text)\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Word2Vec(all_words, size=100, window=20, min_count=1, workers=4)\n",
    "model2 = Word2Vec(all_words, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorize(kwlist, model):\n",
    "    ans = np.zeros(shape=(100,))\n",
    "    count = 0\n",
    "    for i in kwlist:\n",
    "        i = re.sub('[^a-zA-Z]', '',i)\n",
    "        try:\n",
    "            ans += model.wv[i.lower()]\n",
    "            count +=1\n",
    "        except:\n",
    "            continue\n",
    "    return ans/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectorized = pd.DataFrame()\n",
    "\n",
    "df_vectorized['Spacy_Vectorized-M1'] = df_keywords['Spacy'].apply(Vectorize, args = (model1,))\n",
    "df_vectorized['Yake_Vectorized-M1'] = df_keywords['Yake'].apply(Vectorize, args = (model1,))\n",
    "df_vectorized['RakeNLTK_Vectorized-M1'] = df_keywords['RakeNLTK'].apply(Vectorize, args = (model1,))\n",
    "df_vectorized['Gensim_Vectorized-M1'] = df_keywords['Gensim'].apply(Vectorize, args = (model1,))\n",
    "df_vectorized['Keybert-distilbert_Vectorized-M1'] = df_keywords['Keybert-distilbert'].apply(Vectorize, args = (model1,))\n",
    "df_vectorized['Keybert-paraphrase_Vectorized-M1'] = df_keywords['Keybert-paraphrase'].apply(Vectorize, args = (model1,))\n",
    "\n",
    "df_vectorized['Spacy_Vectorized-M2'] = df_keywords['Spacy'].apply(Vectorize, args = (model2,))\n",
    "df_vectorized['Yake_Vectorized-M2'] = df_keywords['Yake'].apply(Vectorize, args = (model2,))\n",
    "df_vectorized['RakeNLTK_Vectorized-M2'] = df_keywords['RakeNLTK'].apply(Vectorize, args = (model2,))\n",
    "df_vectorized['Gensim_Vectorized-M2'] = df_keywords['Gensim'].apply(Vectorize, args = (model2,))\n",
    "df_vectorized['Keybert-distilbert_Vectorized-M2'] = df_keywords['Keybert-distilbert'].apply(Vectorize, args = (model2,))\n",
    "df_vectorized['Keybert-paraphrase_Vectorized-M2'] = df_keywords['Keybert-paraphrase'].apply(Vectorize, args = (model2,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Step 4:  Summarizing Paragraphs to generate fixed length Answers</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sen):\n",
    "    stop_words = stopwords.words('english')\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new\n",
    "\n",
    "def Custom_Summary(sentences,n):\n",
    "    word_embeddings = {}\n",
    "    sentence_vectors = []\n",
    "    summary = ''\n",
    "    \n",
    "    f1 = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "    for line in f1:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    clean_sentences = pd.Series(sentences).str.strip().replace(\"[^a-zA-Z]\", \" \")   \n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "    stop_words = stopwords.words('english')\n",
    "    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "    \n",
    "    for i in clean_sentences:\n",
    "        if len(i) != 0:\n",
    "            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v = np.zeros((100,))\n",
    "        sentence_vectors.append(v)\n",
    "    \n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "    \n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    for i in range(n):\n",
    "        summary = summary + ranked_sentences[i][1] + ' '\n",
    "\n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gensim_Summary(sentences,wc):\n",
    "    contents = ' '.join(sentences)\n",
    "    return summarize(contents, word_count= wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LexRank_Summary(sentences,n):\n",
    "    LANG = \"english\"\n",
    "    ans = ''\n",
    "    contents = ' '.join(sentences)\n",
    "    parser = PlaintextParser.from_string(contents, Tokenizer(LANG))\n",
    "    summarizer = LexRankSummarizer()\n",
    "    summary = summarizer(parser.document, n) \n",
    "    for sentence in summary:\n",
    "        ans = ans + str(sentence) + ' '\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Luhn_Summary(sentences,n):\n",
    "    LANG = \"english\"\n",
    "    ans = ''\n",
    "    contents = ' '.join(sentences)\n",
    "    parser = PlaintextParser.from_string(contents, Tokenizer(LANG))\n",
    "    summarizer = LuhnSummarizer()\n",
    "    summary = summarizer(parser.document, n) \n",
    "    for sentence in summary:\n",
    "        ans = ans + str(sentence) + ' '\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSA_Summary(sentences,n):\n",
    "    LANG = \"english\"\n",
    "    ans = ''\n",
    "    contents = ' '.join(sentences)\n",
    "    parser = PlaintextParser.from_string(contents, Tokenizer(LANG))\n",
    "    summarizer = LsaSummarizer()\n",
    "    summary = summarizer(parser.document, n) \n",
    "    for sentence in summary:\n",
    "        ans = ans + str(sentence) + ' '\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sent(text_data):\n",
    "    sents = nltk.sent_tokenize(text_data)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent = pd.DataFrame(sent_data, columns = ['Chapter Name', 'Paragraph Title', 'Sentence'])\n",
    "df_para = pd.DataFrame(para_data, columns = ['Chapter Name', 'Paragraph Title', 'Description'])\n",
    "df_sent.index = np.arange(1,len(df_sent)+1)\n",
    "df_para.index = np.arange(1,len(df_para)+1)\n",
    "df_summarized = pd.DataFrame()\n",
    "df_summarized['Sentence List'] = df_para['Description'].apply(make_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summarized['Custom Summarizer'] = df_summarized['Sentence List'].apply(Custom_Summary, args = (3,))\n",
    "df_summarized['Gensim Summarizer'] = df_summarized['Sentence List'].apply(Gensim_Summary, args = (50,))\n",
    "df_summarized['LexRank Summarizer'] = df_summarized['Sentence List'].apply(LexRank_Summary, args = (3,))\n",
    "df_summarized['Luhn Summarizer'] = df_summarized['Sentence List'].apply(Luhn_Summary, args = (3,))\n",
    "df_summarized['LSA Summarizer'] = df_summarized['Sentence List'].apply(LSA_Summary, args = (3,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Step 5: Query Question to Vector</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorize_Question(question, method = 'Spacy', model = model2):\n",
    "    if (method == 'Yake'):\n",
    "        question_kw = yake_kw_ext(question)\n",
    "    elif (method == 'Rakenltk'):\n",
    "        question_kw = rakenltk_kw_ext(question)\n",
    "    elif (method == 'Gensim'):\n",
    "        question_kw = gensim_kw_ext(question)\n",
    "    elif (method == 'Kb-distil'):\n",
    "        question_kw = KeyBert_distilbert(question)\n",
    "    elif (method == 'Kb-para'):\n",
    "        question_kw = KeyBert_paraphrase(question)\n",
    "    else:\n",
    "        question_kw = spacy_kw_ext(question)\n",
    "    #print(question_kw)\n",
    "    question_vec = Vectorize(question_kw, model)    \n",
    "    return question_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Step 6: Cosine Similiarity to find required paragraph</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Para_Vec_Scorer( question_vec, model, method = 'Spacy'):\n",
    "    para_id = 0\n",
    "    para_score = []\n",
    "    \n",
    "    if (method == 'Yake'):\n",
    "        mvec = 'Yake_Vectorized'\n",
    "    elif (method == 'RakeNLTK'):\n",
    "        mvec = 'RakeNLTK_Vectorized'\n",
    "    elif (method == 'Gensim'):\n",
    "        mvec = 'Gensim_Vectorized'\n",
    "    elif (method == 'Keybert-distilbert'):\n",
    "        mvec = 'Keybert-distilbert_Vectorized'\n",
    "    elif (method == 'Keybert-paraphrase'):\n",
    "        mvec = 'Keybert-paraphrase_Vectorized'\n",
    "    else:\n",
    "        mvec = 'Spacy_Vectorized'\n",
    "        \n",
    "    if (model == 'Model1'):\n",
    "        mvec = mvec + '-' + 'M1'\n",
    "    else:\n",
    "        mvec = mvec + '-' +'M2'   \n",
    "    for idx,vec in enumerate(df_vectorized[mvec]):\n",
    "        curr = np.dot(question_vec, vec) / (np.linalg.norm(question_vec) * np.linalg.norm(vec))\n",
    "        para_score.append(curr)\n",
    "    return para_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Para_Title_Scorer(q):\n",
    "    title_tokens = []\n",
    "    q_tokens = []\n",
    "    para_title_score = [0]\n",
    "    sw_list = ['where', 'what', 'why', 'which', 'how', 'describe', 'details', 'detailed', 'list', 'give']\n",
    "\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.extend(sw_list)\n",
    "\n",
    "    for i in df_para['Paragraph Title']:\n",
    "        i = i.translate(str.maketrans('', '', string.punctuation))\n",
    "        doc = nlp(i)\n",
    "        title_tokens.append([token.lemma_.lower() for token in doc])\n",
    "\n",
    "    q = q.translate(str.maketrans('', '', string.punctuation))\n",
    "    doc = nlp(q)\n",
    "    for token in doc:\n",
    "        if token.lemma_ not in all_stopwords:\n",
    "            q_tokens.append(token.lemma_.lower())\n",
    "    q_tokens\n",
    "\n",
    "    for i in title_tokens:\n",
    "        for j in q_tokens:\n",
    "            if j in i:\n",
    "                para_title_score[-1]+=1\n",
    "        para_title_score[-1] /= len(q_tokens)\n",
    "        para_title_score.append(0)\n",
    "        \n",
    "    return para_title_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Para(para_score, para_title_score):\n",
    "    final_score = []\n",
    "    for i in range(len(para_score)):\n",
    "        final_score.append(para_score[i] + para_title_score[i])\n",
    "    \n",
    "    return final_score.index(max(final_score)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Answer(para_id,summary_method):\n",
    "    temp = summary_method + ' Summarizer'\n",
    "    if para_id == 0:\n",
    "        return 'No Valid Answer found'\n",
    "    return df_summarized[temp][para_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Dark orange'>Complete Implementation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mSUBJECTIVE ANSWER FINDER\u001b[0m\n",
      "Please type your Query Question: How was stone wood and bones used by hunter gatherers ?\n",
      "\n",
      "Please select a keyword Extraction method:\n",
      "\u001b[32m1.Spacy\t\t 2.Yake\t\t 3.RakeNLTK\t\t 4.Gensim\t\t 4.KB-distilbert\t\t 5.KB-paraphrase\u001b[0m\n",
      "1\n",
      "\n",
      "Please select a Summarization method:\n",
      "\u001b[32m1.Custom\t\t 2.Gensim\t\t 3.LexRank\t\t 4.Luhn\t\t 5.LSA\u001b[0m\n",
      "2\n",
      "\n",
      "Please select a Model\n",
      "\u001b[32m1.M1 --> More Selective\t\t 2.M2 --> Less Selective\u001b[0m\n",
      "2\n",
      "\u001b[34m\n",
      "\n",
      "Your choices are:\u001b[0m\n",
      "Question:  How was stone wood and bones used by hunter gatherers ?\n",
      "Keyword Extractor:  Spacy\n",
      "Summarizer:  Gensim\n",
      "Selected Model:  model2\n",
      "\n",
      "Do you wish to proceed (y or n): y\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "\u001b[34mComplete Answer\u001b[0m\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "How do we know about this place ?\n",
      "Archaeologists have found some of the things hunter gatherers made and used. It is likely that people made and used tools of stone, wood and bone, of which stone tools have survived best. Some of these stone tools were used to cut meat and bone, scrape bark (from trees) and hides (animal skins), chop fruit and roots. Some may have been attached to handles of bone or wood, to make spears and arrows for hunting. Other tools were used to chop wood, which was used as firewood. Wood was also used to make huts and tools.\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\u001b[34mSummarized Answer:\u001b[0m\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "How do we know about this place ?\n",
      "It is likely that people made and used tools of stone, wood and bone, of which stone tools have survived best.\n",
      "Other tools were used to chop wood, which was used as firewood.\n",
      "Wood was also used to make huts and tools.\n"
     ]
    }
   ],
   "source": [
    "kw_dict = {1: \"Spacy\", 2: \"Yake\", 3: \"RakeNLTK\", 4: \"Gensim\", 5: \"Keybert-distilbert\", 6: \"Keybert-paraphrase\"}\n",
    "sum_dict = {1: \"Custom\", 2: \"Gensim\", 3: \"LexRank\", 4: \"Luhn\", 5: \"LSA\"}\n",
    "model_dict = {1: \"model1\", 2: \"model2\"}\n",
    "model_dict_2 = {1: model1, 2: model2}\n",
    "\n",
    "\n",
    "print(colored('SUBJECTIVE ANSWER FINDER','blue'))\n",
    "Question = input('Please type your Query Question: ')\n",
    "\n",
    "print('\\nPlease select a keyword Extraction method:')\n",
    "while True:\n",
    "    print(colored('1.Spacy\\t\\t 2.Yake\\t\\t 3.RakeNLTK\\t\\t 4.Gensim\\t\\t 4.KB-distilbert\\t\\t 5.KB-paraphrase','green'))\n",
    "    try:\n",
    "        Kw_Extraction_Method = int(input())\n",
    "    except ValueError:\n",
    "        print(colored(\"Please enter Valid Choice\", 'red'))\n",
    "        continue\n",
    "    if Kw_Extraction_Method not in [1,2,3,4,5]:\n",
    "        print(colored('Please Choose a valid Keyword Extraction Method', 'red'))\n",
    "        continue\n",
    "    break\n",
    "print('\\nPlease select a Summarization method:')\n",
    "while True:\n",
    "    print(colored('1.Custom\\t\\t 2.Gensim\\t\\t 3.LexRank\\t\\t 4.Luhn\\t\\t 5.LSA', 'green'))\n",
    "    try:\n",
    "        Summarization_Method = int(input())\n",
    "    except ValueError:\n",
    "        print(colored(\"Please enter Valid Choice\", 'red'))\n",
    "        continue\n",
    "    if Summarization_Method not in [1,2,3,4,5]:\n",
    "        print(colored('Please Choose a valid Summarizer', 'red'))\n",
    "        continue\n",
    "    break\n",
    "print('\\nPlease select a Model')\n",
    "while True:\n",
    "    print(colored('1.M1 --> More Selective\\t\\t 2.M2 --> Less Selective', 'green'))\n",
    "    try:\n",
    "        Chosen_Model = int(input())\n",
    "    except ValueError:\n",
    "        print(colored(\"Please enter Valid Choice\", 'red'))\n",
    "        continue\n",
    "    if Chosen_Model not in [1,2]:\n",
    "        print(colored('Please Choose correct Model', 'red'))\n",
    "        continue\n",
    "    break\n",
    "print(colored('\\n\\nYour choices are:', 'blue'))\n",
    "print(\"Question: \", Question)\n",
    "print(\"Keyword Extractor: \", kw_dict[Kw_Extraction_Method])\n",
    "print(\"Summarizer: \", sum_dict[Summarization_Method])\n",
    "print(\"Selected Model: \", model_dict[Chosen_Model])\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        choice1 = input('\\nDo you wish to proceed (y or n): ')\n",
    "    except ValueError:\n",
    "        print(colored(\"Please enter Valid Choice\", 'red'))\n",
    "        continue\n",
    "    choice1 = choice1.lower()\n",
    "    if choice1 not in ['y','n']:\n",
    "        print(colored('Please choose a Valid Choice', 'red'))\n",
    "        continue\n",
    "    break\n",
    "    \n",
    "if (choice1 == 'n'):\n",
    "    print(colored('\\nRun Again to Change Choices', 'red'))\n",
    "else:\n",
    "    qvec = Vectorize_Question(question = Question, model = model_dict_2[Chosen_Model], method = kw_dict[Kw_Extraction_Method])\n",
    "    p_sc = Para_Vec_Scorer(qvec, model_dict[Chosen_Model], method = kw_dict[Kw_Extraction_Method])\n",
    "    pt_sc = Para_Title_Scorer(Question)\n",
    "    pid = Find_Para(p_sc, pt_sc)\n",
    "    answer = Get_Answer(pid, sum_dict[Summarization_Method])\n",
    "    \n",
    "    print(\"-------------------------------------------------------------------------------------------------------------\")\n",
    "    print(colored('Complete Answer', 'blue'))\n",
    "    print(\"-------------------------------------------------------------------------------------------------------------\")\n",
    "    print(df_para['Paragraph Title'][pid])\n",
    "    print(df_para['Description'][pid])\n",
    "    \n",
    "    print(\"--------------------------------------------------------------------------------------------------------------\")    \n",
    "    print(colored('Summarized Answer:', 'blue'))\n",
    "    print(\"-------------------------------------------------------------------------------------------------------------\")\n",
    "    print(df_para['Paragraph Title'][pid])\n",
    "    print(answer)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_dict = {1: \"Spacy\", 2: \"Yake\", 3: \"RakeNLTK\", 4: \"Gensim\", 5: \"Keybert-distilbert\", 6: \"Keybert-paraphrase\"}\n",
    "sum_dict = {1: \"Custom\", 2: \"Gensim\", 3: \"LexRank\", 4: \"Luhn\", 5: \"LSA\"}\n",
    "model_dict = {1: \"model1\", 2: \"model2\"}\n",
    "model_dict_2 = {1: model1, 2: model2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Spacy 1 1\n",
      "1 Spacy 12 2\n",
      "2 Spacy 3 3\n",
      "3 Spacy 5 5\n",
      "4 Spacy 6 6\n",
      "5 Spacy 7 7\n",
      "6 Spacy 8 8\n",
      "7 Spacy 9 9\n",
      "8 Spacy 3 15\n",
      "9 Spacy 2 5\n",
      "10 Spacy 16 16\n",
      "11 Spacy 17 17\n",
      "12 Spacy 11 11\n",
      "13 Spacy 18 18\n",
      "14 Spacy 12 18\n",
      "15 Spacy 2 13\n",
      "16 Spacy 2 12\n",
      "17 Spacy 15 8\n",
      "18 Spacy 17 17\n",
      "19 Spacy 1 1\n",
      "13  out of 20:   Spacy\n",
      "----------------------------------------\n",
      "0 Yake 1 1\n",
      "1 Yake 12 2\n",
      "2 Yake 4 3\n",
      "3 Yake 5 5\n",
      "4 Yake 6 6\n",
      "5 Yake 7 7\n",
      "6 Yake 8 8\n",
      "7 Yake 9 9\n",
      "8 Yake 2 15\n",
      "9 Yake 2 5\n",
      "10 Yake 15 16\n",
      "11 Yake 17 17\n",
      "12 Yake 11 11\n",
      "13 Yake 8 18\n",
      "14 Yake 15 18\n",
      "15 Yake 2 13\n",
      "16 Yake 2 12\n",
      "17 Yake 1 8\n",
      "18 Yake 17 17\n",
      "19 Yake 1 1\n",
      "10  out of 20:   Yake\n",
      "----------------------------------------\n",
      "0 RakeNLTK 1 1\n",
      "1 RakeNLTK 15 2\n",
      "2 RakeNLTK 8 3\n",
      "3 RakeNLTK 5 5\n",
      "4 RakeNLTK 6 6\n",
      "5 RakeNLTK 7 7\n",
      "6 RakeNLTK 8 8\n",
      "7 RakeNLTK 9 9\n",
      "8 RakeNLTK 2 15\n",
      "9 RakeNLTK 7 5\n",
      "10 RakeNLTK 16 16\n",
      "11 RakeNLTK 17 17\n",
      "12 RakeNLTK 13 11\n",
      "13 RakeNLTK 16 18\n",
      "14 RakeNLTK 12 18\n",
      "15 RakeNLTK 13 13\n",
      "16 RakeNLTK 2 12\n",
      "17 RakeNLTK 18 8\n",
      "18 RakeNLTK 5 17\n",
      "19 RakeNLTK 1 1\n",
      "10  out of 20:   RakeNLTK\n",
      "----------------------------------------\n",
      "0 Gensim 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-57a7a7374ef8>:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ans/count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Gensim 12 2\n",
      "2 Gensim 3 3\n",
      "3 Gensim 17 5\n",
      "4 Gensim 1 6\n",
      "5 Gensim 1 7\n",
      "6 Gensim 8 8\n",
      "7 Gensim 1 9\n",
      "8 Gensim 4 15\n",
      "9 Gensim 5 5\n",
      "10 Gensim 1 16\n",
      "11 Gensim 17 17\n",
      "12 Gensim 1 11\n",
      "13 Gensim 18 18\n",
      "14 Gensim 15 18\n",
      "15 Gensim 1 13\n",
      "16 Gensim 2 12\n",
      "17 Gensim 16 8\n",
      "18 Gensim 17 17\n",
      "19 Gensim 1 1\n",
      "8  out of 20:   Gensim\n",
      "----------------------------------------\n",
      "0 Keybert-distilbert 1 1\n",
      "1 Keybert-distilbert 12 2\n",
      "2 Keybert-distilbert 3 3\n",
      "3 Keybert-distilbert 5 5\n",
      "4 Keybert-distilbert 6 6\n",
      "5 Keybert-distilbert 7 7\n",
      "6 Keybert-distilbert 12 8\n",
      "7 Keybert-distilbert 9 9\n",
      "8 Keybert-distilbert 1 15\n",
      "9 Keybert-distilbert 5 5\n",
      "10 Keybert-distilbert 16 16\n",
      "11 Keybert-distilbert 17 17\n",
      "12 Keybert-distilbert 11 11\n",
      "13 Keybert-distilbert 18 18\n",
      "14 Keybert-distilbert 12 18\n",
      "15 Keybert-distilbert 2 13\n",
      "16 Keybert-distilbert 2 12\n",
      "17 Keybert-distilbert 15 8\n",
      "18 Keybert-distilbert 5 17\n",
      "19 Keybert-distilbert 1 1\n",
      "12  out of 20:   Keybert-distilbert\n",
      "----------------------------------------\n",
      "0 Keybert-paraphrase 1 1\n",
      "1 Keybert-paraphrase 12 2\n",
      "2 Keybert-paraphrase 4 3\n",
      "3 Keybert-paraphrase 5 5\n",
      "4 Keybert-paraphrase 18 6\n",
      "5 Keybert-paraphrase 7 7\n",
      "6 Keybert-paraphrase 3 8\n",
      "7 Keybert-paraphrase 9 9\n",
      "8 Keybert-paraphrase 2 15\n",
      "9 Keybert-paraphrase 5 5\n",
      "10 Keybert-paraphrase 16 16\n",
      "11 Keybert-paraphrase 17 17\n",
      "12 Keybert-paraphrase 13 11\n",
      "13 Keybert-paraphrase 18 18\n",
      "14 Keybert-paraphrase 15 18\n",
      "15 Keybert-paraphrase 2 13\n",
      "16 Keybert-paraphrase 6 12\n",
      "17 Keybert-paraphrase 1 8\n",
      "18 Keybert-paraphrase 5 17\n",
      "19 Keybert-paraphrase 1 1\n",
      "9  out of 20:   Keybert-paraphrase\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Testing The accuracy:\n",
    "\n",
    "#kw_dict = {1: \"Spacy\", 2: \"Yake\"}\n",
    "kw_dict = {1: \"Spacy\", 2: \"Yake\", 3: \"RakeNLTK\", 4: \"Gensim\", 5: \"Keybert-distilbert\", 6: \"Keybert-paraphrase\"}\n",
    "sum_dict = {1: \"Custom\", 2: \"Gensim\", 3: \"LexRank\", 4: \"Luhn\", 5: \"LSA\"}\n",
    "model_dict = {1: \"model2\"}\n",
    "model_dict_2 = {1: model2}\n",
    "\n",
    "q1 = \"Give details of Tushar's journey\"\n",
    "q2 = 'Describe the life of Hunter Gatherers.'\n",
    "q3 = 'How was stone wood and bones used by hunter gatherers ?'\n",
    "q4 = 'Give some information about Rock paintings'\n",
    "q5 = 'What are sites ?'\n",
    "q6 = 'Give uses of fire'\n",
    "q7 = 'What is Palaeolithic period ?'\n",
    "q8 = 'What were the changes in climate and environment ?'\n",
    "q9 = 'Discuss Making Stone Tools by the people of Stone Age.'\n",
    "q10 = 'Discuss in short the art of Rock Paintings done by the people of the early stone age.'\n",
    "q11 = 'Describe life in Mehrgarh'\n",
    "q12 = 'Give details of Cave paintings in France.'\n",
    "q13 = 'Which plants and animals were selected for domestication ?'\n",
    "q14 = 'Name a famous neolithic site.'\n",
    "q15 = 'Life at Catal Huyuk'\n",
    "q16 = 'How were animals of help to people ?'\n",
    "q17 = 'How did growing plants affect location of people ?'\n",
    "q18 = 'Explain the terms Palaeolithic Age and Mesolithic age.'\n",
    "q19 = 'Which animals fomred a major part of cave paintings ?'\n",
    "q20 = 'Where was Tushar travelling ?'\n",
    "\n",
    "a1 = 1\n",
    "a2 = 2\n",
    "a3 = 3\n",
    "a4 = 5\n",
    "a5 = 6\n",
    "a6 = 7\n",
    "a7 = 8\n",
    "a8 = 9\n",
    "a9 = 15\n",
    "a10 = 5\n",
    "a11 = 16\n",
    "a12 = 17\n",
    "a13 = 11\n",
    "a14 = 18\n",
    "a15 = 18\n",
    "a16 = 13\n",
    "a17 = 12\n",
    "a18 = 8\n",
    "a19 = 17\n",
    "a20 = 1\n",
    "\n",
    "\n",
    "q_list = [q1,q2,q3,q4,q5,q6,q7,q8,q9,q10,q11,q12,q13,q14,q15,q16,q17,q18,q19,q20]\n",
    "a_list = [a1,a2,a3,a4,a5,a6,a7,a8,a9,a10,a11,a12,a13,a14,a15,a16,a17,a18,a19,a20]\n",
    "for kw_met in kw_dict.values():\n",
    "    correct_ans = 0\n",
    "    for i in range(len(q_list)):\n",
    "        for model_met in model_dict_2.values():\n",
    "            qvec = Vectorize_Question(question = q_list[i], model = model_met, method = kw_met)\n",
    "            p_sc = Para_Vec_Scorer(qvec, model_met, method = kw_met)\n",
    "            pt_sc = Para_Title_Scorer(q_list[i])\n",
    "            pid = Find_Para(p_sc, pt_sc)\n",
    "            \n",
    "            print(i, kw_met, pid, a_list[i])\n",
    "            if pid == a_list[i]:\n",
    "                correct_ans +=1\n",
    "    print(correct_ans,' out of 20:  ', kw_met)\n",
    "    print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Testing The accuracy:\n",
    "\n",
    "#kw_dict = {1: \"Spacy\", 2: \"Yake\"}\n",
    "kw_dict = {1: \"Spacy\", 2: \"Yake\", 3: \"RakeNLTK\", 4: \"Gensim\", 5: \"Keybert-distilbert\", 6: \"Keybert-paraphrase\"}\n",
    "sum_dict = {1: \"Custom\", 2: \"Gensim\", 3: \"LexRank\", 4: \"Luhn\", 5: \"LSA\"}\n",
    "model_dict = {1: \"model2\"}\n",
    "model_dict_2 = {1: model2}\n",
    "\n",
    "q1 = \"Give details of Tushar's journey\"\n",
    "q2 = 'Describe the life of Hunter Gatherers.'\n",
    "q3 = 'How was stone wood and bones used by hunter gatherers ?'\n",
    "q4 = 'Give some information about Rock paintings'\n",
    "q5 = 'What are sites ?'\n",
    "q6 = 'Give uses of fire'\n",
    "q7 = 'What is Palaeolithic period ?'\n",
    "q8 = 'What were the changes in climate and environment ?'\n",
    "q9 = 'Discuss Making Stone Tools by the people of Stone Age.'\n",
    "q10 = 'Discuss in short the art of Rock Paintings done by the people of the early stone age.'\n",
    "q11 = 'Describe life in Mehrgarh'\n",
    "q12 = 'Give details of Cave paintings in France.'\n",
    "q13 = 'Which plants and animals were selected for domestication ?'\n",
    "q14 = 'Name a famous neolithic site.'\n",
    "q15 = 'Life at Catal Huyuk'\n",
    "q16 = 'How were animals of help to people ?'\n",
    "q17 = 'How did growing plants affect location of people ?'\n",
    "q18 = 'Explain the terms Palaeolithic Age and Mesolithic age.'\n",
    "q19 = 'Which animals fomred a major part of cave paintings ?'\n",
    "q20 = 'Where was Tushar travelling ?'\n",
    "\n",
    "a1 = 1\n",
    "a2 = 2\n",
    "a3 = 3\n",
    "a4 = 5\n",
    "a5 = 6\n",
    "a6 = 7\n",
    "a7 = 8\n",
    "a8 = 9\n",
    "a9 = 15\n",
    "a10 = 5\n",
    "a11 = 16\n",
    "a12 = 17\n",
    "a13 = 11\n",
    "a14 = 18\n",
    "a15 = 18\n",
    "a16 = 13\n",
    "a17 = 12\n",
    "a18 = 8\n",
    "a19 = 17\n",
    "a20 = 1\n",
    "\n",
    "\n",
    "q_list = [q1,q2,q3,q4,q5,q6,q7,q8,q9,q10,q11,q12,q13,q14,q15,q16,q17,q18,q19,q20]\n",
    "a_list = [a1,a2,a3,a4,a5,a6,a7,a8,a9,a10,a11,a12,a13,a14,a15,a16,a17,a18,a19,a20]\n",
    "for kw_met in kw_dict.values():\n",
    "    correct_ans = 0\n",
    "    for i in range(len(q_list)):\n",
    "        for model_met in model_dict_2.values():\n",
    "            qvec = Vectorize_Question(question = q_list[i], model = model_met, method = kw_met)\n",
    "            p_sc = Para_Vec_Scorer(qvec, model_met, method = kw_met)\n",
    "            pt_sc = [0]*len(p_sc)\n",
    "            pid = Find_Para(p_sc, pt_sc)\n",
    "            \n",
    "            print(i, kw_met, pid, a_list[i])\n",
    "            if pid == a_list[i]:\n",
    "                correct_ans +=1\n",
    "            \n",
    "    print(correct_ans,' out of 20:  ', kw_met)\n",
    "    print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
